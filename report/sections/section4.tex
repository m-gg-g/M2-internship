 \section{Randomness in transitions}
 	In this section, we first define the generalized model and update the definition of oriented games to account for the randomness introduced in the state transition function. Then, for these new games, we prove a result equivalent to those presented in Theorem \ref{theorem_iid_and_oriented_percolation_games}.

	\subsection{Generalizing the model}
	For a percolation game $\Gamma = (I, J, \mathcal{E}, g, q)$, we consider a generalization of the model by adding a discrete-time stochastic process $\xi = (\xi_m)_{m \geq 1}$ to the game's transition function $q$, where $(\xi_m)$ is a sequence of i.i.d. discrete random variables defined on the same probability space $(\Omega, \mathcal{F}, \mathbb{P})$. In the sequel, this game is denoted by $\Gamma_{\xi} = (I, J, \mathcal{E}, g, q + \xi)$, and for a specific known common distribution $\mu$ of the $(\xi_m)$, it is denoted by $\Gamma_{\mu}$. 
	
	Given an initial state $z \in \Z^d$  and $\omega \in \Omega$, the game $\Gamma_{\xi}$ proceed in the same way as in $\Gamma$: Players are informed of $\omega$. At each stage $m \in \N_+$, player 1 chooses an action $i_m$, and then knowing player 1's action, player 2 chooses an action $j_m$. Players 1 and 2 receive respective payoffs $g_w(z_m, i_m, j_m)$ and $-g_w(z_m, i_m, j_m)$. Then, the new state updates according to the modified transition function 
	\[
		z_{m+1} := z_m + q(i_m, j_m) + \xi_{m}.
	\]
	
	Since the payoff random variables do not change, $\Gamma_{\xi}$ will be i.i.d. as long as $\Gamma$ is i.i.d. On the other side, it is necessary to adjust the definition of oriented game. To handle the randomness in the state's transition function, we required the product to be positive in expectation.
	
	\begin{definition} A game $\Gamma_{\xi}$ is oriented if there exists $u \in \Z^d$ such that for all $(i, j) \in I \times J$ and $m \in \N_+$, $q(i, j)\cdot u + \E(\xi_m \cdot u) > 0$.
	\end{definition}
	
	\subsection{Main result}
	We consider a game $\Gamma_{\xi}$ where $\xi$ affects only the first component, i.e., for all $m \geq 1$,
	\begin{equation}\label{stochasticprocess-condition1}
	 	[\xi_m(\omega)]_k = 0, \text{ for all } k \geq 2 \text{ and } \omega \in \Omega.
	\end{equation} 
	\noindent We further assume that $([\xi_m]_1)_{m \geq 1}$ has zero expectation and bounded support, specifically, for all $m \geq 1$,
	\begin{equation}\label{stochasticprocess-condition2}
    	\E([\xi_m]_1) = 0 \text{ and } \Pbb(a \leq [\xi_m]_1 \leq b) = 1 \text{ for some } a, b \in \mathbb{R} \text{ with } a < b.
	\end{equation}

	Note that for such stochastic process $\xi$, if $\Gamma$ is oriented, then $\Gamma_{\xi}$ is also oriented, since $\E(\xi_m \cdot u) = u_1 \E([\xi_m]_1) = 0$ for all $m \geq 1$ and any vector $u \in \mathbb{Z}^d$.

	Similarly as for $\Gamma$ i.i.d. and oriented, we claim the following for $\Gamma_{\xi}$.
	
	\begin{theorem} \label{theorem-main}
		Consider a percolation game $\Gamma_{\xi}$ with $\Gamma$ i.i.d. and oriented, and $(\xi_m)$ a sequence of i.i.d. discrete random variables satisfying \eqref{stochasticprocess-condition1} and \eqref{stochasticprocess-condition2}. For all $z \in \Z^d$, $v_n(z)$ converges $\mathbb{P}$-a.s. to a constant $v_{\infty} \in \R$, as $n$ tends to infinity. Moreover, there exists $A, B > 0$ such that, for all $n \geq 1$, $z \in \Z^d$ and $\lambda \geq 0$,
		\begin{equation}\label{theorem-rate-of-Lonvergence}
			\Pbb\left(|v_n(z) - v_{\infty}| \geq \lambda + A\ln(n+1)^{1/2}n^{-1/2}\right) \leq \exp\left(-B\lambda^2n\right).
		\end{equation}
		In particular, $\E(v_n)$ converges to $v_{\infty}$ at a rate of $O(\ln(n)^{1/2}n^{-1/2})$.
	\end{theorem}
		
	Before proving Theorem \ref{theorem-main}, we will first introduce some notations and concepts, along with two lemmas that will be used in its proof.

	Denote $\|q\|_{\infty} := \sup_{(i, j, k) \in I \times J \times [d]} [q(i, j)]_k$, $\|g\|_{\infty} := \sup_{(\omega, z, i, j) \in \Omega \times \Z^d \times I \times J} |g_{\omega}(z, i, j)|$ and $\|q + \xi\|_{\infty} := \|q\|_{\infty} + \sup_{(\omega, m, k) \in \Omega  \times \N \times [d]}|[\xi_m(\omega)]_k|$. 
	
	Let $n \geq 1$, define $H := \{z \in \Z^d \mid z \cdot u = 0\},$ the orthogonal hyperplane to $u$, and 
	$$\mathscr{C} := [-n\cdot\|q+\xi\|_{\infty}..n\cdot\|q+\xi\|_{\infty}]^d$$ a discrete cube where the $n$-stage game, starting from the origin of $\Z^d$, stays in. 
	
	There exists a constant $C$ depending only on $u$ and $d$, such that $\mathscr{C}$ can be covered by a collection $(H_r)_{r \in [1..Cn]}$ of discrete hyperplanes that are translations of $H$: for each $r \in [1..Cn]$, $H_r = \{z \in \Z^d \mid z \cdot u = h_r\}$ for some $h_r \in \Z$, and $\mathscr{C} \subset \cup_{r = 1}^{Cn}H_r$. 
		
	For $r \in [1..Cn]$, let $\F_r := \sigma(\{\omega \mapsto g_{\omega}(z, i, j)\}\colon (z, i, j) \in \cup_{r' = 1}^{r}H_{r'}\times I \times J),$
	and let $\F_0$ be the trivial $\sigma-$algebra.

	In the sequel, for $r \in [1..Cn]$ and a pair of strategies $(\sigma, \tau) \in \Sigma \times T$ we will be interested in how many times the token crosses the hyperplane $H_{r+1}$, or in other words, how many times the token returns to an already visited hyperplane perpendicular to $u$. 

	An initial position 
	$z_1 \in \mathscr{C}$ and a pair of strategies $(\sigma, \tau) \in \Sigma \times T$, induces along with $q$ and $(\xi_m)$ a probability distribution $\Pbb_{z_1, \sigma, \tau}$ over the set of plays $(\Z^d \times I \times J)^{\infty}$, endowed with the product $\sigma$-algebra. Explicitly, given $h_m = (z_1, i_1, j_1, \ldots, z_{m-1}, i_{m-1}, j_{m-1}, z_m)$ we have:
	\[
		\Pbb_{z_1, \sigma, \tau}(h_m) = \prod_{k = 2}^{m}\mathbf{1}_{\left\{\substack{\tau_{k-1}(h_{k-1}, i_{k-1}) = j_{k-1}, \\ \sigma_{k-1}(h_{k-1}) = i_{k-1}} \right\}}\Pbb(\xi_{k - 1} + z_{k-1} + q(i_{k-1}, j_{k-1}) = z_k),
	\]
	which extends to infinite plays via the Kolmogorov extension theorem.

	We will denote the expectation with respect to $\Pbb_{z_1, \sigma, \tau}$ by $\E_{z_1, \sigma, \tau}$.

	For $r \in [1..Cn]$ and $k \in [n]$, let $A_{k, r}$ be the event: ``the token returns to the hyperplane $H_{r+1}$ on the $k$-stage'' and $I_{k, r}$ be the indicator random variable of the event $A_{k, r}$, i.e.
	\[
		I_{k, r} = \begin{cases}
				1  & \text{the token returns to $H_{r+1}$ on the $k$-stage}, \\
				0  & \text{otherwise}.
			  \end{cases}
	\]
	Let $M_{n, r}$ be the random variable representing the number of times the token returns to $H_{r+1}$ up to stage $n$. It can be expressed as
	\[
		M_{n, r} = \sum_{k = 1}^{n} I_{k, r},
	\]
	and its expected value as 
	\begin{equation}\label{expected-mn}
	\E_{z_1, \sigma, \tau}(M_{n, r}) = \sum_{k = 1}^{n} \E_{z_1, \sigma, \tau}(I_{k, r}) = \sum_{k = 1}^{n} \Pbb_{z_1, \sigma, \tau}(A_{k, r}).
	\end{equation}

	For the expectation of $M_{n, r}$, we claim the following upper bound.

\begin{lemma} \label{finite-number-of-Lrossings}
	Let $z_1 \in \mathscr{C}$, $(\sigma, \tau) \in \Sigma \times T$, $r \in [1..Cn]$, and $n \in \N_+$. There exists a constant $L > 0$, depending only on $a$ and $b$, such that
	\begin{equation}\label{EMn}
		\E_{z_1, \sigma, \tau}(M_{n, r}) \leq \frac{\exp(-L)}{1 - \exp(-L)}.
	\end{equation}
\end{lemma}

	\begin{proof} Let $r \in [1..Cn]$ and $n \in \N_+$.

	At each stage $n$, $\xi$ introduces a random variability in the token's position. However, the token already has a positive drift in the constant direction $u$, which consistently pushes it away from every hyperplane perpendicular to $u$ that it crosses. Therefore, there are two competing forces: the movement in the direction of $u$ and the random fluctuations caused by $\xi$. 

	Given this persistent drifts in the direction of $u$ and the bounded range of fluctuations of $\xi$, as the token progresses --or, in other words, beyond a certain distance from the hyperplane-- it is expected that the probability of crossing it again becomes very small. 

	To cause a return, the cumulative random fluctuations introduce by $\xi$ would need to overcome an increasingly large deterministic displacement and the probability of this happening decreases exponentially with the number of stages. This is what we prove next, after which we use \eqref{expected-mn} to derive the desired result.

	%Therefore, it is expected that, as $n$ increases, the event $A_n$ becomes increasingly unlikely. t.

	Let $s$ be the stage at which the token first crosses the hyperplane $H_{r+1}$. At each stage $k \in [n]$, if $s \leq k$, the total magnitude of the displacement of the token in the direction $u$ from the hyperplane $H_{r+1}$ can be expressed as
	\[
		d_k = \sum_{m=s}^k \|\vectproj[u]{q(i_m, j_m)}\|.
	\]
	If $s > k$, we make $d_k = 0$ since in this case the token has not even cross for the first time the hyperplane $H_{r+1}$. This sum grows linearly with the number of stages.

	Given $s \leq k$, for the token to cross $H_{r+1}$ again, the random component, given by the cumulative effect of $\xi$, must at least overcome the distance $d_k$. Therefore, the probability of the event $A_{k, r}$ is bounded by
	\[
		\Pbb_{z_1, \sigma, \tau}(A_{k, r}) = \Pbb_{z_1, \sigma, \tau}(\text{the token returns after distance} \; d_k) \leq \Pbb\left(\sum_{m=s}^k [\xi_m]_1 > d_k\right).
	\]
	Applying Hoeffding's inequality, we obtain
	$$
		\Pbb\left(\sum_{m=s}^k [\xi_m]_1 > d_k\right) \leq \exp\left(-\frac{d_k^2}{2(b - a)^2(k-s)}\right).{}
	$$
	Since $d_k = O(k)$, for some constant $L > 0$, we have
	$$
		\Pbb\left(\sum_{m=s}^k [\xi_m]_1 > d_k\right) \leq \exp\left(-Lk\right).
	$$
	If $s > k$, $I_{k, r} = 0$ and therefore $\Pbb_{z_1, \sigma, \tau}(A_{k, r}) = 0$. 

	Consequently, by \eqref{expected-mn} we obtain that $\E_{z_1, \sigma, \tau}(M_{n, r})$ is bounded by a geometric sum with ratio equal to $\exp\left(-L\right)$,
	\[
		\E_{z_1, \sigma, \tau}(M_{n, r}) \leq \sum_{k = 1}^n \exp\left(-L\right)^k = \exp\left(-L\right)\frac{1 - \exp\left(-Ln\right)}{1 - \exp\left(-L\right)} \leq \frac{\exp\left(-L\right)}{1 - \exp\left(-L\right)}.
	\]
	\end{proof}

	% \begin{proof} \textbf{(using the SLLN and the asymptotics of the expected maximum distance of the random walk)}

	% 	In the scenario we consider, a random walk is added to one of the components (in our particular case, the first component) of the trajectory of the token, which is determined by a deterministic function $q$. This trajectory already moves in a fixed direction $u$. Therefore, the overall movement of the token can be viewed as a combination of moves in direction $u$ and random fluctuations. Specifically, at each stage $n \in \N_+$, the position of the token can be expressed as 
	% 	\[
	% 			z_{n} = z_0 + \sum_{m=1}^nq(i_m, j_m) + \sum_{m=1}^n \xi_m =: z_0 + Q_n + R_n, 
	% 	\]
	% 	where $R_n$ represents the cummulative effect of the random walk and $Q_n$ the cummulative effect of the deterministic moves induces by the transition function. 

	% 	By the Strong Law of Large Numbers, 
	% 	\[
	% 		\frac{R_n}{n} \longrightarrow 0 \quad \text{a.s.}, \; \text{as } \; n \to \infty.
	% 	\] 
	% 	This says that, over a long period $n$, the average movement will be dominated by the deterministic drift $Q_n$, as the random fluctuations tend to cancel out.

	% 	Let's consider the projection of the vector $q(i_m, j_m)$ onto the vector $u$: $\vectproj[u]{q(i_m, j_m)}$. The norm of this projection represents the magnitude of the displacement in the direction of $u$ at stage $m$. After $n$ stages, the total magnitude of the displacement is given by:
	% 	\[
	% 		\sum_{m=1}^n \|\vectproj[u]{q(i_m, j_m)}\|.
	% 	\]
	% 	This sum grows linearly with the number of stages, while the random component introduces fluctuations that grow only at a rate of $O(\sqrt{n})$. Consequently, the random component does not grow fast enough to counteract the linear growth of the deterministic component. 

	% 	All this suggests that for large $n$, the drift in the direccion $u$ dominates: the cummulative random fluctutations $R_n$ become negligible compared to $Q_n$. This indicates that eventually, the token moves away in the direction of $u$, and hence, it moves away from any fixed point. Therefore, with probability 1, there exists a finite time after which the token will never return to the hyperplane $H_{r+1}$ again. Consequently, the total number of crossings of $H_{r+1}$ is finite almost surely.

	% 	%The displacement in the direction of $u$ grows linearly with the number of steps, while the random fluctuations grow only as $O(\sqrt{n})$. This ensures that the token will almost surely stop crossing the hyperplane after some finite number of stages.

	% 	To summarize: the deterministic component provides a consistent positive drift in the direction of $u$, and the random walk component adds fluctuations. However, the variability of these fluctuations grows as $O(\sqrt{n})$, which is slower than the advance in the direction of $u$, which is $O(n)$. The only issue would be if the progress in the direction of $u$ decreases with respect to the number of stages $n$, but this does not happen. I mean, the sum $\sum_{m=1}^n \|\vectproj[u]{q(i_m, j_m)}\|$ must grow as $O(n)$ or more; for $O(n)$ growth, it is sufficient that $|q(i, j)| \leq C$, for all $(i, j)$ and some constant $C > 0$. This not clear in the model, or yes? Should we assume it at the beginning? We also use it when define and use $\|q\|_{\infty}$.

	% \end{proof}

	% Note that while the token can theoretically cross any given hyperplane multiple times, the probability of doing so decreases rapidly with distance due to the persistent drift in a specific direction. We have just proven that the actual number of crossings for any specific realization of the walk is finite with probability 1.

	The second lemma provides a key inequality for the proof of Theorem \ref{theorem-main}.

		\begin{lemma} \label{lemma2}
			For $r \in [1..Cn]$ and some constant $D > 0$, it holds
			\begin{equation}\label{bound-Lond-v}
				|\E (v_n(0)|\F_{r+1}) - \E (v_n(0)|\F_r)| \leq \frac{2D\|g\|_{\infty}}{n} \quad \mathbb{P}\text{-a.s.}
			\end{equation}
		\end{lemma}
	\begin{proof}
	For $r \in [1..Cn]$, define an auxiliary game $\Gamma_{\xi}'$ with the same actions sets and transition function as $\Gamma_{\xi}$, and payoff function defined for all $(\omega, i, j) \in \Omega \times I \times J$ by
	\[
		g'_{\omega}(z, i, j) = \begin{cases}
								0				   &	 z \in H_{r+1},\\
								g_{\omega}(z, i, j) & \text{otherwise} .
							  \end{cases}
	\]
	For $\omega \in \Omega$, let $\gamma_n^{'\omega}$ be the $n$-stage payoff, and $v_n^{'\omega}$ the $n$-stage value. 
	
	Because $\Gamma$ is oriented, without the fluctuations that random increments bring, regardless of player's strategies, the token would be  in the hyperplane $H_{r+1}$ at most once. For $\Gamma_{\xi}$, the token still tends to move in the direction of $u$, but its progress will be slower since it can backtrack and pass through previously visited hyperplanes. The number of times this occurs up to stage $n$ is $M_{n, r}$. Therefore, there can be at most $M_{n, r} + 1$ stages where the state lies in $H_{r+1}$. It follows that, for all $(\sigma, \tau) \in \Sigma \times T$,
	\[
		|\gamma_n^{'\omega}(0, \sigma, \tau) - \gamma_n^{\omega}(0, \sigma, \tau)| \leq \frac{(1+M_{n, r})\|g\|_{\infty}}{n}. 
	\]
	Hence
	\[
		|v_n'(0) - v_n(0)| \leq \frac{(1 + M_{n, r})\|g\|_{\infty}}{n}, \quad \mathbb{P}\text{-a.s.}
	\]
	Taking conditional expectation with respect to $\F_{r+1}$, since $M_{n, r}$ is independent of $\F_{r+1}$ 
	\[
		\E\left(|v_n'(0) - v_n(0)|\mid \F_{r+1}\right) \leq \frac{(1 + \E(M_{n, r}))\|g\|_{\infty}}{n},
	\]
	and applying Jensen inequality to the left hand-side term, we obtain
	\[
		|\E(v_n'(0)|\F_{r+1}) - \E(v_n(0)|\F_{r+1})| \leq \frac{(1 + \E(M_{n, r}))\|g\|_{\infty}}{n}.
	\]
	By lemma \ref{finite-number-of-Lrossings}, $\E(M_{n, r}) = O(1)$. Hence, there exists a $D > 0$, such that
	\[
		|\E(v_n'(0)|\F_{r+1}) - \E(v_n(0)|\F_{r+1})| \leq \frac{D\|g\|_{\infty}}{n}.
	\]

	Moreover, $(v_n'(0), \F_r)$ is independent of the r.v's $\{\omega \mapsto g_{\omega}(z, i, j)\}_{(z, i, j) \in H_{r+1} \times I \times J}$ because $\Gamma_{\xi}$ is i.i.d. It follows that $\E(v_n'(0)|\F_{r+1}) = \E(v_n'(0)|\F_{r})$ $\mathbb{P}$-a.s.. Hence, by summing and subtracting this term in the left hand side of \eqref{bound-Lond-v} and using the previous inequality, we obtain the claimed inequality
	\[
		|\E (v_n(0)|\F_{r+1}) - \E (v_n(0)|\F_r)| \leq \frac{2D\|g\|_{\infty}}{n} \quad \mathbb{P}\text{-a.s.}
	\] 
	\end{proof}

	We will now continue with the remainder of the proof of Theorem \ref{theorem-main}. From this point onward, we primarily follow the proof provided by Garnier and Ziliotto for Theorem 2.1 in \cite{GarnierZiliotto2022}.

	\begin{proof}
		Let $W_r := \E(v_n(0)|\F_r), r\in [0..Cn]$. For every $r\in [0..Cn]$, $W_r$ is integrable since $v_n(0)$ is integrable, as the random variables $\{\omega \mapsto g_{\omega}(z, i, j)\}_{(z, i, j) \in \in H_{r+1} \times I \times J}$ are uniformly bounded. By the tower property $\E(W_{r+1}|\F_r) = W_r$, and by definition is $\F_r$-measurable. It follows that the process $(W_r)_{r \in [0..Cn]}$ is a martingale, with $W_0 = \E(v_n(0))$. By the stationary assumption, the expectation of $v_n(z)$ is independent of $z$; thus, we will write $\E(v_n)$ instead of $\E(v_n(0))$. Moreover, starting from 0, $z_n$ remains within $\mathscr{C}$, so $v_n(0)$ is $\F_{Cn}-$measurable, and thus $W_{Cn} = v_n(0)$. 

		Consequently, by Azuma–Hoeffding's inequality and \eqref{bound-Lond-v}, for all $\lambda \geq 0$,
		\begin{equation}\label{azuma}
			\mathbb{P}(|v_n(0) - \E(v_n)| \geq \lambda) \leq \exp\left(\frac{-\lambda^2n}{8K\|g\|_{\infty}^2}\right),
		\end{equation}
	where $K = CD^2 > 0$.

	This results tell us that $v_n(0)$ is concentrated around its mean as $n$ increases.
		
	On other hand, by the union bound, we have
	\begin{eqnarray*}
		\Pbb(\exists z \in \mathscr{C}, |v_n(z) - \E(v_n)| \geq \lambda) & = & \Pbb\left(\bigcup_{z\in \mathscr{C}}\{|v_n(z) - \E(v_n)| \geq \lambda\}\right) \\
		 & \leq & \sum_{z\in \mathscr{C}}\mathbb{P}(|v_n(z) - \E(v_n)| \geq \lambda).
	\end{eqnarray*}
	Since the variables $v_n(z)$ and $v_n(0)$ have the same distribution due to stationarity, the right hand side is equal to
	\[
		\sum_{z\in \mathscr{C}}\mathbb{P}(|v_n(z) - \E(v_n)| \geq \lambda) = |\mathscr{C}| \cdot \mathbb{P}(|v_n(0) - \E(v_n)| \geq \lambda). 
	\]
	
	Using inequality \eqref{azuma} and bounding $|\mathcal{C}|$, we obtain
	\begin{eqnarray*}
		\Pbb(\exists z \in \mathscr{C}, |v_n(z) - \E(v_n)| \geq \lambda) & \leq & |\mathscr{C}| \cdot \mathbb{P}(|v_n(0) - \E(v_n)| \geq \lambda) \\
		 & \leq & (2n \cdot \|q + \xi \|_{\infty} + 1)^d \exp\left(\frac{-\lambda^2n}{8K\|g\|_{\infty}^2}\right).
	\end{eqnarray*}
 
	% \textcolor{magenta}{I'm sending this to you up to this point because I'm very interested in your feedback and, of course, in knowing if it's correct. If it is, for the remaining steps is just to follow the proof in your paper, as the resulting inequality \eqref{bound-Lond-v} is the same, except for a constant.}

	Taking $\lambda = \lambda_n := 2\sqrt{2K} \|g\|_{\infty} \ln (2n\|q + \xi\|_{\infty} + 1)^{1/2}(d+1)^{1/2}n^{-1/2}$, we obtain 
	$$ 
		\Pbb(\exists z \in \mathscr{C}, |v_n(z) - \E(v_n)| \geq \lambda_n)  \leq (2n \cdot \|q + \xi \|_{\infty} + 1)^{-1} \leq n^{-1}.
	$$
	and hence,
	\begin{eqnarray*}
		\Pbb(\exists z \in \mathscr{C}, |v_n(z) - \E(v_n)| \geq \lambda_n) & \geq & \Pbb(\exists z \in \mathscr{C}, \E(v_n) - v_n(z) \geq \lambda_n) \\
		& = & \Pbb\left(\min_{z\in \mathscr{C}}v_n(z) \leq \E(v_n) - \lambda_n\right). \\
	\end{eqnarray*}
	From where, we have 
	\begin{equation}\label{min-n^-1}
		\Pbb\left(\min_{z\in \mathscr{C}}v_n(z) \leq \E(v_n) - \lambda_n\right) \leq n^{-1}. 
	\end{equation}

	On the other hand, from the law of total expectation, we have
	\begin{eqnarray*}
		\E\left(\min_{z\in \mathscr{C}}v_n(z)\right) & = & \E\left(\min_{z\in \mathscr{C}}v_n(z) \big| \min_{z\in \mathscr{C}}v_n(z) \leq \E(v_n) - \lambda_n\right)\Pbb\left(\min_{z\in \mathscr{C}}v_n(z) \leq \E(v_n) - \lambda_n\right) \\
		& & + \;\; \E\left(\min_{z\in \mathscr{C}}v_n(z) \big| \min_{z\in \mathscr{C}}v_n(z) \geq \E(v_n) - \lambda_n\right)\Pbb\left(\min_{z\in \mathscr{C}}v_n(z) \geq \E(v_n) - \lambda_n\right).
	\end{eqnarray*}

	From this, applying Markov inequality, using the fact that $v_n(z) \geq -\|g\|_{\infty}$ for all $z \in \mathscr{C}$, and using \eqref{min-n^-1}, we deduce that
	\begin{eqnarray*}
		\E\left(\min_{z\in \mathscr{C}}v_n(z)\right) & \geq & -\Pbb\left(\min_{z\in \mathscr{C}}v_n(z) \leq \E(v_n) - \lambda_n\right)\|g\|_{\infty} \\
		& & + \;\; \Pbb\left(\min_{z\in \mathscr{C}}v_n(z) \geq \E(v_n) - \lambda_n\right)(\E(v_n) - \lambda_n)\\  
		& = & \Pbb\left(\min_{z\in \mathscr{C}}v_n(z) \leq \E(v_n) - \lambda_n\right)\left(-\|g\|_{\infty} - \E(v_n) + \lambda_n\right) + \E(v_n) - \lambda_n\\ 
		& \geq & -n^{-1}\|g\|_{\infty} - n^{-1}|\E(v_n) - \lambda_n| + \E(v_n) - \lambda_n
	\end{eqnarray*}

	Hence, since $\lambda_n = O(\ln(n+1)^{1/2}n^{-1/2})$, there exists $D > 0$ such that for sufficiently large $n$, we have
	\begin{equation}\label{min-D}
		\E\left(\min_{z\in \mathscr{C}}v_n(z)\right) \geq \E(v_n) - D\ln(n+1)^{1/2}n^{-1/2}.
	\end{equation}

	Now, let's prove that $\E(v_n)$ converges. To do this, we establish the subaditivity of $(-n\E(v_n))_{n \geq 1}$ by analyzing the game in blocks. We then apply inequality \eqref{min-D} and use Theorem 23 from \cite{deBrujinErdos52} to obtain the desired convergence. 

	Let $n \geq 1$ and $m \in [1..n]$. Consider the $(m + n)$-stage game starting from state $z = 0$. Let player 2 plays optimally from $z$ and player 1 plays the following strategy: play optimally in the $m$-stage game, then starting from state $z_{m+1}$ play optimally in the $n$-stage game. At stage $m+1$, the state is in $\mathcal{C}$. Hence, this strategy guarantees 
	$$
		v_{m+n} \geq \frac{mv_m(0) + n\min_{z \in \mathcal{C}}v_n(z)}{n + m} \quad \Pbb\text{-a.s.} 
	$$
	Taking expectation on both sides and using \eqref{min-D}, we obtain
	\begin{eqnarray} \notag
		\E(v_{m+n}) & \geq & \frac{m}{n + m}\E(v_m) + \frac{n}{n + m}\E\left(\min_{z \in \mathcal{C}}v_n(z)\right) \\ \label{last}
					& \geq & \frac{m}{n + m}\E(v_m) + \frac{n}{n + m}\left(\E(v_n) - D\ln(n+1)^{1/2}n^{-1/2}\right).
	\end{eqnarray}
	
	Let $a_n := -n\E(v_n)$ for $n \in \N_+$. By the previous inequality, we have
	\begin{equation*}
		a_{m+n} \leq a_m + a_n + D\ln(n+m+1)^{1/2}(n+m)^{1/2},
	\end{equation*}
	for all $m, n \geq 1$. Setting $f(n) := D\ln(n+1)^{1/2}n^{1/2}$ this gives us that the sequence $(a_n)$ is subaditive with an error term $f$. Furthermore, $f$ is positive, increasing, and satisfies
	\[
		\sum_{n\geq 1}\frac{f(n)}{n^2} = D\sum_{n\geq 1}\frac{\ln (n+1)^{1/2}}{n^{3/2}} < \infty.
	\]
	Therefore, by Theorem 23 of \cite{deBrujinErdos52}, we have $a_n/n \to L$ for some $-\infty \leq L < \infty$. Thus, $\E(v_n)$ converges. Let $v_{\infty}$ denote its limit. 

	To obtain the rate of convergence, we use inequality \eqref{last}. By induction, this inequality gives us that for all $l \geq 1$ and $n \geq 1$, 
	\begin{eqnarray} \notag
		\E(v_{2^ln}) & \geq & \E(v_n) - \frac{D}{2} \sum_{k = 1}^{l}\ln(2^{k-1}n + 1)^{1/2}2^{-(k-1)/2}n^{-1/2}\\
					 & \geq & \E(v_n) - A \ln(n + 1)^{1/2}n^{-1/2},
	\end{eqnarray}
	where $A$ is a positive constant. 

	Taking the limit as $l$ tends to infinity, we obtain that for all $n \geq 1$,
	\[
		v_{\infty} \geq \E(v_n) - A\ln(n + 1)^{1/2}n^{-1/2}.
	\]
	
	Returning to the analysis where we play the game in blocks and exchanging the roles of player 1 and player 2, we obtain the following result for all $n \geq 1$,
	\[
		|v_{\infty} - \E(v_n)| \leq A\ln(n + 1)^{1/2}n^{-1/2}.
	\]

	Combining this inequality with inequality \eqref{azuma}, we have
	\[
		\Pbb(|v_n(0) - v_{\infty}| \geq \lambda + A\ln(n+1)^{1/2}n^{-1/2}) \leq \exp\left(\frac{-\lambda^2n}{8k\|g\|_{\infty}^2}\right).
	\]
	
	Finally, by stationarity, the same inequality holds for $v_n(z)$ for any $z \in \Z^d$. Thus, inequality \eqref{theorem-rate-of-Lonvergence} holds for $B := (8k\|g\|_{\infty}^2)^{-1}$. 

	\end{proof}

	As a particular case, we can add a random walk with a large step. For example, let $(\xi_m)$ be such that $\Pbb(\xi_m = e_1) = p, \Pbb(\xi_m = -e_1) = q$ and $\Pbb(\xi_m = Ne_1) = r$, where $p, q, r \in [0, 1]$ and $p + q + r = 1$, with $N$ being a large integer much greater than 1 and $r = (q - p)/N$.

	Additionally, we could consider a symmetric simple random walk affecting one of the coordinates chosen at random (i.e., a simple random walk on $\Z^d$). Specifically, let $(\xi_m)$ be such that $\mathbb{P}(\xi_m = e_k) = \mathbb{P}(\xi_m = -e_k) = 1/{2d}$ for all $k \in [d]$, where $e_k$ denotes the $k$-th unit vector in $\Z^d$.
